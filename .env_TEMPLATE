GEMINI_API_KEY='YOUR_API_KEY_HERE'
GROK_API_KEY='your_api_key_here'
ANTHROPIC_API_KEY='your-api-key-here'

### qwen
OPENAI_API_KEY="none"
# ifconfig: docker0 network
# make sure your local ollama/ llama allows connection from all hosts e.g.:
# ./llama-server -m ./qwen3:4b --n-gpu-layers 99 --jinja --host 0.0.0.0
OPENAI_BASE_URL="http://172.17.0.1:8080/v1"
OPENAI_MODEL="qwen3"